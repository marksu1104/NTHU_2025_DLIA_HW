{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "609dcb62-c2f8-4c6d-9c89-63dc0148a87c"
   },
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "###### Lab 2\n",
    "\n",
    "# National Tsing Hua University\n",
    "\n",
    "#### Spring 2025\n",
    "\n",
    "#### 11320IEEM 513600\n",
    "\n",
    "#### Deep Learning and Industrial Applications\n",
    "    \n",
    "## Lab 2: Predicting Heart Disease with Deep Learning\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "061c22d2-eec4-40f4-866b-ccaa2d9a2963",
    "tags": []
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In the realm of healthcare, early detection and accurate prediction of diseases play a crucial role in patient care and management. Heart disease remains one of the leading causes of mortality worldwide, making the development of effective diagnostic tools essential. This lab leverages deep learning to predict the presence of heart disease in patients using a subset of 14 key attributes from the Cleveland Heart Disease Database. The objective is to explore and apply deep learning techniques to distinguish between the presence and absence of heart disease based on clinical parameters.\n",
    "\n",
    "Throughout this lab, you'll engage with the following key activities:\n",
    "- Use [Pandas](https://pandas.pydata.org) to process the CSV files.\n",
    "- Use [PyTorch](https://pytorch.org) to build an Artificial Neural Network (ANN) to fit the dataset.\n",
    "- Evaluate the performance of the trained model to understand its accuracy.\n",
    "\n",
    "### Attribute Information\n",
    "\n",
    "1. age: Age of the patient in years\n",
    "2. sex: (Male/Female)\n",
    "3. cp: Chest pain type (4 types: low, medium, high, and severe)\n",
    "4. trestbps: Resting blood pressure\n",
    "5. chol: Serum cholesterol in mg/dl\n",
    "6. fbs: Fasting blood sugar > 120 mg/dl\n",
    "7. restecg: Resting electrocardiographic results (values 0,1,2)\n",
    "8. thalach: Maximum heart rate achieved\n",
    "9. exang: Exercise induced angina\n",
    "10. oldpeak: Oldpeak = ST depression induced by exercise relative to rest\n",
    "11. slope: The slope of the peak exercise ST segment\n",
    "12. ca: Number of major vessels (0-3) colored by fluoroscopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversible defect\n",
    "14. target: target have disease or not (1=yes, 0=no)\n",
    "\n",
    "### References\n",
    "- [UCI Heart Disease Data](https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data) for the dataset we use in this lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "ad594fc8-4989-40f3-b124-4550fe7df386"
   },
   "source": [
    "## A. Checking and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1742459869268,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "42a3eafd-cbcd-4c56-82cb-83a0bfa2399e",
    "outputId": "7738e53e-1638-4a0b-9593-28218b70b5d7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('heart_dataset_train_all.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1742459872390,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "34241797-60f0-4818-a44b-f5379948d621",
    "outputId": "8e99d293-0074-4a5f-b54f-194c950b1aa8"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1742459876545,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "026585db-a6d8-4062-85de-e3a7eaebed72",
    "outputId": "917b0427-e824-4216-d399-210a3f371b4c"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1742459880934,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "69031e6d-0fb5-49d9-b723-a0d1fee08c3c",
    "outputId": "1e2c63dc-7d17-4a67-b520-066e4e2d543f"
   },
   "outputs": [],
   "source": [
    "# checking for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1742459884914,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "cb3090f8-2cfa-4f56-8aa5-cf954bb19932"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1742459887745,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "38aadbee-d68f-4ae0-b842-b40800b0cac9",
    "outputId": "003a8e11-f0f8-497c-ad83-d86d5ca64d58"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1742459890842,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "26a69fd5-3534-4d8e-b59a-6778bf47a479",
    "outputId": "e1cb7ec5-4b02-41d0-f803-ce0609093ce8"
   },
   "outputs": [],
   "source": [
    "# Mapping 'sex' descriptions to numbers\n",
    "sex_description = {\n",
    "    'Male': 0,\n",
    "    'Female': 1,\n",
    "}\n",
    "df.loc[:, 'sex'] = df['sex'].map(sex_description)\n",
    "\n",
    "# Mapping 'cp' (chest pain) descriptions to numbers\n",
    "pain_description = {\n",
    "    'low': 0,\n",
    "    'medium': 1,\n",
    "    'high': 2,\n",
    "    'severe': 3\n",
    "}\n",
    "df.loc[:, 'cp'] = df['cp'].map(pain_description)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1742459894534,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "051108c6-7011-4187-9e36-bd2944a019ca",
    "outputId": "0c1fee4b-ea24-4f22-da93-4fed17fd1f39"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1742459896347,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "8b999df5-09a1-4ce2-b068-f1afba448ff8",
    "outputId": "a6cd327f-0679-4ccd-945c-77c271a885bb"
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "8ce7a0c5-76d6-4863-ba61-0544a220962a"
   },
   "source": [
    "#### Converting the DataFrame to a NumPy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1742459900409,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "5735baad-2db8-4306-aa4c-7788d2b49621",
    "outputId": "e5634c85-5f40-413f-be09-1ac0b7f11e4b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_data = df.values\n",
    "np_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1742460305286,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "29b8e189-7f39-435a-8038-39098b147325"
   },
   "outputs": [],
   "source": [
    "split_point = int(np_data.shape[0]*0.7)\n",
    "\n",
    "np.random.shuffle(np_data)\n",
    "\n",
    "x_train = np_data[:split_point, :13]\n",
    "y_train = np_data[:split_point, 13]\n",
    "x_val = np_data[split_point:, :13]\n",
    "y_val = np_data[split_point:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1742460355125,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "fedb56d7-1665-4c90-9697-b86cab43f300",
    "outputId": "7df32384-60f7-4ef7-b92e-2b372fb3e14b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train = np.array(x_train, dtype=float)\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = np.array(y_train, dtype=int)\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n",
    "x_val = np.array(x_val, dtype=float)\n",
    "x_val = torch.from_numpy(x_val).float()\n",
    "y_val = np.array(y_val, dtype=int)\n",
    "y_val = torch.from_numpy(y_val).long()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'Number of samples in train and validation are {len(train_loader.dataset)} and {len(val_loader.dataset)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "8ffc26b9-6044-41e9-93e2-7dc6250dbd27"
   },
   "source": [
    "## B. Defining Neural Networks\n",
    "\n",
    "In PyTorch, we can use **class** to define our custom neural network architectures by subclassing the `nn.Module` class. This gives our neural network all the functionality it needs to work with PyTorch's other utilities and keeps our implementation organized.\n",
    "\n",
    "- Neural networks are defined by subclassing `nn.Module`.\n",
    "- The layers of the neural network are initialized in the `__init__` method.\n",
    "- The forward pass operations on input data are defined in the `forward` method.\n",
    "\n",
    "It's worth noting that while we only define the forward pass, PyTorch will automatically derive the backward pass for us, which is used during training to update the model's weights.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742460379394,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "77975746-a7a7-4676-9527-57674cd98c0f"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(13, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "cbb8b5b0-0ec0-406c-a42e-048aa00e05aa"
   },
   "source": [
    "## C. Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89,
     "status": "ok",
     "timestamp": 1742460381570,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "3602ae7d-4034-4c49-b221-0c12a5824b18",
    "outputId": "f4673995-b14d-4aa0-e1be-c8cc3174ee9d"
   },
   "outputs": [],
   "source": [
    "# Check your GPU status.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b3afc8c576604557b0ed34eccd7fcb01",
      "be82a0b273b148e188655d84d479b5c6",
      "d9fac97e7da246f1b2c514d4dbaaab69",
      "167886efe3934f939f2667e85c708f25",
      "0b3b617d2f774d0d98d945aadf21a10b",
      "46ed422cdd4f43f7b46f0f05902cf772",
      "1b8fa50895034830b53497c4bbec0880",
      "757a0fae785f4c3aa9f4dd2af6a1d24e",
      "8f765c76896a4008a03945b2c8d63078",
      "0b32611e586442e091574299abf92844",
      "96017968444e4e1caaeec5c3fc0f5c9e"
     ]
    },
    "executionInfo": {
     "elapsed": 8072,
     "status": "ok",
     "timestamp": 1742460400523,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "f73a5c35-c15d-49bb-8a33-a7f017159499",
    "outputId": "832be6ac-c9ab-4baa-c253-dba67572b0c8"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "model = Model()\n",
    "# print(model)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = -1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = model(features)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_predicted = outputs.argmax(-1)\n",
    "        train_correct += (train_predicted == labels).sum().item()\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "    # Learning rate update\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100. * train_correct / total_train_samples\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features = features.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = model(features)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            predicted = outputs.argmax(-1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = 100. * correct / total\n",
    "\n",
    "    # Checkpoint\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), 'model_classification.pth')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train loss: {avg_train_loss:.4f}, Train acc: {train_accuracy:.4f}%, Val loss: {avg_val_loss:.4f}, Val acc: {val_accuracy:.4f}%, Best Val loss: {best_val_loss:.4f} Best Val acc: {best_val_acc:.2f}%')\n",
    "\n",
    "    # Store performance\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "a7984c6e-6652-4160-b572-07d48bc93a3f"
   },
   "source": [
    "#### Visualizing the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1742460406150,
     "user": {
      "displayName": "林明聰",
      "userId": "10350944269371174646"
     },
     "user_tz": -480
    },
    "id": "5559d850-1fb5-4b04-b6ca-60c5b309f34e",
    "outputId": "148f2fc0-462b-464a-d933-6c8aa8b1864b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plotting training and validation accuracy\n",
    "ax[0].plot(train_accuracies)\n",
    "ax[0].plot(val_accuracies)\n",
    "ax[0].set_title('Model Accuracy')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(['Train', 'Val'])\n",
    "\n",
    "# Plotting training and validation loss\n",
    "ax[1].plot(train_losses)\n",
    "ax[1].plot(val_losses)\n",
    "ax[1].set_title('Model Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(['Train', 'Val'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "89c7e51b-8ab6-4aa2-877d-39b6daf55c20"
   },
   "source": [
    "## D. Evaluating Your Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "f49735d7-466f-4037-8078-172f03dffd8d",
    "outputId": "e815ef48-71b9-4f9d-98d8-61e3ca1ac734"
   },
   "outputs": [],
   "source": [
    "# read test file\n",
    "test_data = pd.read_csv('heart_dataset_test.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21ae9d85-0dc2-4db0-a7c7-807c6b6c514f",
    "outputId": "d43458a1-1a66-47f6-fcdb-6111da6be2d9"
   },
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ff2812b-a5a5-4ea9-86be-ae2143cb2ba7",
    "outputId": "d143e201-d151-4d90-cabc-9719fb87b48f"
   },
   "outputs": [],
   "source": [
    "test_data = test_data.values\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "14d4be20-f64f-421d-8971-e1e47873aef8"
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "x_test = torch.from_numpy(test_data[:, :13]).float()\n",
    "y_test = torch.from_numpy(test_data[:, 13]).long()\n",
    "\n",
    "# Create datasets\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bcf8580-42ee-4ee7-ad15-9f080cc57a33",
    "outputId": "8b2ea734-c445-46e2-b24b-c911b86e1ba0"
   },
   "outputs": [],
   "source": [
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('model_classification.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = model(features)\n",
    "\n",
    "        predicted = outputs.argmax(-1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f'Test accuracy is {100. * test_correct / test_total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## E. Hyperparameter Select and Experiments\n",
    "\n",
    "Select 2 hyper-parameters of the artificial neural network used in Lab 2 and set 3 different values for each. Perform experiments to compare the effects of varying these hyper-parameters on the loss and accuracy metrics across the training, validation, and test datasets. Present your findings with appropriate tables.\n",
    "\n",
    "**Hyperparameter**\n",
    "- `hidden_units` = { 256, 512, 1024 }\n",
    "- `hidden_layers` = { 5, 10, 50 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size=13, hidden_units=256, hidden_layers=2, num_classes=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        # First hidden layer\n",
    "        layers.append(nn.Linear(input_size, hidden_units))\n",
    "        layers.append(nn.ReLU())\n",
    "        # Additional hidden layers if any\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_units, hidden_units))\n",
    "            layers.append(nn.ReLU())\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_units, num_classes))\n",
    "        self.model = nn.Sequential(*layers).cuda()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 100  # training epochs configuration\n",
    "\n",
    "# Store experiment results\n",
    "results = []\n",
    "\n",
    "# Enumerate the nine combinations\n",
    "hidden_units_options = [256, 512, 1024]\n",
    "hidden_layers_options = [5, 10, 50]\n",
    "\n",
    "for hidden_units in hidden_units_options:\n",
    "    for hidden_layers in hidden_layers_options:\n",
    "        print(f'\\n==================\\nStarting training: hidden_units={hidden_units}, hidden_layers={hidden_layers}')\n",
    "        # Initialize model (ensure Model supports parameters hidden_units and hidden_layers)\n",
    "        model = Model(hidden_units=hidden_units, hidden_layers=hidden_layers).cuda()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_val_acc = -1\n",
    "        \n",
    "        # Record metrics from the last epoch (optionally record every epoch)\n",
    "        for epoch in tqdm(range(epochs), desc=f'(units={hidden_units}, layers={hidden_layers})'):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            train_correct = 0\n",
    "            total_train_samples = 0\n",
    "\n",
    "            for features, labels in train_loader:\n",
    "                features = features.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_pred = outputs.argmax(dim=-1)\n",
    "                train_correct += (train_pred == labels).sum().item()\n",
    "                total_train_samples += labels.size(0)\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            train_accuracy = 100. * train_correct / total_train_samples\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            total_val_loss = 0.0\n",
    "            correct_val = 0\n",
    "            total_val_samples = 0\n",
    "            with torch.no_grad():\n",
    "                for features, labels in val_loader:\n",
    "                    features = features.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                    outputs = model(features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    total_val_loss += loss.item()\n",
    "                    pred_val = outputs.argmax(dim=-1)\n",
    "                    correct_val += (pred_val == labels).sum().item()\n",
    "                    total_val_samples += labels.size(0)\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            val_accuracy = 100. * correct_val / total_val_samples\n",
    "            \n",
    "            # Save best validation performance (adjust checkpoint saving condition as necessary)\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                torch.save(model.state_dict(), 'model_classification.pth')\n",
    "            \n",
    "            # print(f'Epoch {epoch+1}/{epochs}, Train loss: {avg_train_loss:.4f}, Train acc: {train_accuracy:.2f}%, '\n",
    "            #       f'Val loss: {avg_val_loss:.4f}, Val acc: {val_accuracy:.2f}%')\n",
    "        \n",
    "        # Testing phase\n",
    "        model.eval()\n",
    "        total_test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        total_test_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in test_loader:\n",
    "                features = features.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_test_loss += loss.item()\n",
    "                pred_test = outputs.argmax(dim=-1)\n",
    "                test_correct += (pred_test == labels).sum().item()\n",
    "                total_test_samples += labels.size(0)\n",
    "        avg_test_loss = total_test_loss / len(test_loader)\n",
    "        test_accuracy = 100. * test_correct / total_test_samples\n",
    "\n",
    "        # Save the final performance for this hyperparameter configuration (using last epoch metrics)\n",
    "        results.append({\n",
    "           'hidden_units': hidden_units,\n",
    "           'hidden_layers': hidden_layers,\n",
    "           'Train Loss': avg_train_loss,\n",
    "           'Train Accuracy (%)': train_accuracy,\n",
    "           'Val Loss': avg_val_loss,\n",
    "           'Val Accuracy (%)': val_accuracy,\n",
    "           'Test Loss': avg_test_loss,\n",
    "           'Test Accuracy (%)': test_accuracy\n",
    "        })\n",
    "        \n",
    "# Create a result table\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n===== Experiment Results =====\")\n",
    "print(df_results)\n",
    "df_results.to_csv(\"experiment_results.csv\", index=False)\n",
    "\n",
    "# Plot charts for each metric as a function of hidden_layers, with different lines for each hidden_units\n",
    "metrics = [\"Train Loss\", \"Train Accuracy (%)\", \"Val Loss\", \"Val Accuracy (%)\", \"Test Loss\", \"Test Accuracy (%)\"]\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for ax, metric in zip(axs, metrics):\n",
    "    for hidden_units in hidden_units_options:\n",
    "        subset = df_results[df_results[\"hidden_units\"] == hidden_units]\n",
    "        ax.plot(subset[\"hidden_layers\"], subset[metric], marker='o', label=f\"units={hidden_units}\")\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel(\"hidden_layers\")\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DL_py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
